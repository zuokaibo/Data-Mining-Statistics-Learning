---
title: "HW3"
author: "Bo"
date: "2026-02-16"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# check correlation and plot them using corrplot package
library(corrplot)

# split dataset using caTools pacakge, 
library(caTools)

# apply LDA  and QDA 
library(MASS)

# creating confusion matrix
library(rlang)
library(ggplot2)
library(caret)

# check equal covirance between classes 
library(heplots)

#check if LDA or QDA fit, using partimat to see partition plots
library(klaR)

# Naive Bayes
library(e1071)

#logistic model
library(dplyr)

# apply ROC curve
library(pROC) 

#apply knn
library(class)

# check Multicollinearity
library(car)

```


```{r cars}
data = read.csv("Auto.csv")
head(data)
#str(data)
```


```{r pressure, echo=FALSE}
median(data$mpg)
# make a indicator that value bigger than 22.75 and smaller than 22.75
# then use as.integer to transform False  True value obtained from flag to 0 and 1.
flag=I(data$mpg>=22.75)
mpg01 = as.numeric(flag)
```


```{r}
par(mfrow = c(1,2))
plot(data$displacement, data$mpg)
plot(data$horsepower, data$mpg)

par(mfrow = c(1,2))
plot(data$weight, data$mpg)
plot(data$acceleration, data$mpg)
```

```{r}
par(mfrow=c(2,2))
hist(data$displacement)
hist(data$horsepower)
hist(data$weight)
hist(data$acceleration)

par(mfrow=c(1,2))
hist(data$mpg)
hist(mpg01)
```


```{r}
corrplot(round(cor(data) ,2))
```


```{r}
pairs(data[, 3:6])
```


```{r}
standardize=function(x){
  z=(x-min(x))/(max(x)-min(x))
  return(z)
}
```


```{r}
stand.data = as.data.frame(apply(data[, 3:6],2,standardize))
#head(stand.data)
data = cbind(data[, 1:2], stand.data, data[, 7:8])
#head(data)
linear.full.model = lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + year + origin, data)
vif(linear.full.model)
summary(linear.full.model)
```


```{r}
linear.reduced.model = lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration, data)
summary(linear.reduced.model)
```



```{r}
anova(linear.reduced.model, linear.full.model)
```



```{r}
selection <- stepAIC(linear.model, direction = "both")  
summary(selection)
```


```{r}
linear.reduced.model.two = lm(mpg ~ cylinders + displacement + horsepower + weight + year + origin, data)
summary(linear.reduced.model.two)
```


```{r}
anova(linear.reduced.model.two, linear.full.model)
```


```{r}
# normalize the numerical data, factorize categorical data. 
newdata = cbind(mpg01, data[,2:5], data[, 7:8]) 
stand.newdata = as.data.frame(apply(newdata[, 3:5],2,standardize))
newdata = cbind(newdata[, 1:2], stand.newdata, newdata[, 6:7])
newdata$mpg01 = as.factor(newdata$mpg01)
newdata$cylinders = as.factor(newdata$cylinders)
newdata$year = as.factor(newdata$year)
newdata$origin = as.factor(newdata$origin)
#str(newdata)
head(newdata)
```


```{r}
split = sample.split(newdata$mpg01, SplitRatio = 0.8)
traindata = subset(newdata, split == TRUE)
testdata = subset(newdata, split == FALSE)
#head(traindata)
```



```{r}
lda.full.model = lda(mpg01~., traindata)
lda.full.model
plot(lda.full.model, dimen=1, type="b")

```

```{r}
lda.full.pred = predict(lda.full.model, newdata=testdata)
#lda.full.pred$class
#testdata$mpg01
lda.matrix = confusionMatrix(data=as.factor(lda.full.pred$class), reference = as.factor(testdata$mpg01))
lda.matrix
```


```{r}
lda.roc = roc(testdata$mpg01, as.numeric(lda.full.pred$class))
plot(lda.roc, col="darkblue", main="Roc curve of LDA classifier")
auc(lda.roc)
```



```{r}
# QDA
qda.model = qda(mpg01~., data=newdata)
#qda.model
```


```{r}
qda.pred = predict(qda.model, newdata=testdata)
#as.numeric(qda.pred$class)
qda.matrix = confusionMatrix(data=as.factor(qda.pred$class), reference = as.factor(testdata$mpg01))
qda.matrix
```


```{r}
qda.roc = roc(testdata$mpg01, as.numeric(qda.pred$class))
plot(qda.roc, col="black", main="Roc curve of QDA classifier")
auc(qda.roc)

```



```{r}
# reset plot margin from bottom, left, top, right order.
par(mar = c(2, 2, 1, 0.5))

lda.partition = partimat(mpg01~as.numeric(cylinders)+displacement+horsepower+weight , testdata, method = "lda")

qda.partition = partimat(mpg01~as.numeric(cylinders)+displacement+horsepower+weight, testdata, method = "qda")
```


```{r}
# using e1071 package to perform naive bayes classifier.
naive.model = naiveBayes(mpg01~., traindata)
#naive.model
```

```{r}
naive.model.pred = predict(naive.model, newdata = testdata)
#as.numeric(naive.model.pred)

naive.matrix = confusionMatrix(testdata$mpg01, as.factor(naive.model.pred))
naive.matrix
```


```{r}
naive.roc <- roc(testdata$mpg01, as.numeric(naive.model.pred))
plot(naive.roc, col = "red", main="Roc curve of Naive Bayes classifier")  
auc(naive.roc)
```


```{r}
#Logistic Regression using dplyr package.
logistic.model = glm (mpg01~., data=traindata, family = "binomial")
summary(logistic.model)

```


```{r}
logistic.pred = predict(logistic.model, newdata=testdata, type = "response")
#logistic.pred

logistic.classification = ifelse(logistic.pred >= 0.5, 1, 0)
head(logistic.classification)

#logistic.matrix = confusionMatrix(testdata$mpg01, as.factor(logistic.pred$class))
#logistic.matrix
```


```{r}
logistic.matrix = confusionMatrix(testdata$mpg01, as.factor(logistic.classification))
logistic.matrix
```


```{r}
logistic.roc <- roc(testdata$mpg01, logistic.pred)  
plot(logistic.roc, col = "brown", main="Roc curve of Logistic Regression classifier")  
auc(logistic.roc)
```


```{r}
#knn
#sqrt(nrow(testdata))
k.values = c(5, 7, 9, 11, 13, 15)

best.k = sapply( k.values, function(k) {
  knn.model = knn(train = traindata, test = testdata, cl=traindata$mpg01, k=k)
  
  1 - mean(knn.model != testdata$mpg01)
} 
) 

ggplot(data.frame(K = k.values, Accuracy = best.k), aes(x = K, y = Accuracy)) +
  geom_line(color = "darkgreen", size = 1) +
  geom_point(color = "brown", size = 3) +
  labs(title = "Model Accuracy for Different K Values",
       x = "Number of Neighbors (K)",
       y = "Accuracy") +
  theme_minimal()

```

```{r}
head(traindata)
best.k = c(5, 7, 9)
knn.pred = sapply( best.k, function(k) {
  knn.model = knn(train = traindata, test = testdata, cl=traindata$mpg01, k=k)
  k.matrix = table(testdata$mpg01, knn.model)
  print(confusionMatrix(k.matrix))
}
)
```

```{r}
#head(traindata)
knn.model = knn(train = traindata, test = testdata, cl=traindata$mpg01, k=7)
k.matrix = table(testdata$mpg01, knn.model)

knn.roc = roc(testdata$mpg01, as.numeric(knn.model))
plot(knn.roc, col="lightblue", main="Roc curve of KNN classifier")
auc(knn.roc)
```


```{r}
newdata.two = cbind(mpg01, data[,2:5], data[, 7:8]) 
stand.newdata.two = as.data.frame(apply(newdata[, 3:5],2,standardize))
newdata.two = cbind(newdata.two[, 1:2], stand.newdata.two, newdata.two[, 6:7])
#str(newdata.two)
newdata.two$cylinders=as.numeric(as.integer(newdata.two$cylinders))
newdata.two$year = as.numeric(as.integer(newdata.two$year))
newdata.two$origin = as.numeric(as.integer(newdata.two$origin))
#str(newdata.two)

pca.model <- prcomp(newdata.two[, 2:7], scale. = FALSE, center = TRUE, retx = TRUE)
pca.model

```

```{r}
pca.var.prop = (pca.model$sdev^2)/sum( (pca.model$sdev^2)  )
pca.var.prop
plot(pca.var.prop, main="PCA analysis", type = "b", xlab = "Number of components", ylab = "Proportion of variance")

```


```{r}
which(cumsum(pca.var.prop) >= 0.9)[1]
```

```{r}
#colnames(newdata.two[, 2:7])
```

```{r}

head(traindata)


k.values.two = c(5, 7, 9, 11, 13, 15)

best.k.two = sapply( k.values.two, function(k) {
  knn.model = knn(train = traindata[, 2:3], test = testdata[, 2:3], cl=traindata$mpg01, k=k)
  
  1 - mean(knn.model != testdata$mpg01)
} 
) 

ggplot(data.frame(K = k.values.two, Accuracy = best.k.two), aes(x = K, y = Accuracy)) +
  geom_line(color = "darkgreen", size = 1) +
  geom_point(color = "brown", size = 3) +
  labs(title = "Model Accuracy for Different K Values",
       x = "Number of Neighbors (K)",
       y = "Accuracy") +
  theme_minimal()
```


```{r}

best.k.two = c(5, 15)
knn.pred.two = sapply( best.k.two, function(k) {
  knn.model = knn(train = traindata[, 2:3], test = testdata[, 2:3], cl=traindata$mpg01, k=k)
  k.matrix = table(testdata$mpg01, knn.model)
  print(confusionMatrix(k.matrix))
}
)
```


```{r}
knn.model.two = knn(train = traindata[,2:3], test = testdata[, 2:3], cl=traindata$mpg01, k=5)
k.matrix = table(testdata$mpg01, knn.model.two)

knn.roc.two = roc(testdata$mpg01, as.numeric(knn.model.two))
plot(knn.roc.two, col="lightblue", main="Roc curve of PCA-KNN")
auc(knn.roc.two)
```









